### –ó–∞–¥–∞–Ω–∏–µ –ê
#### —Ñ—É–Ω–∫—Ü–∏—è normalize
```python
def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    s = text.strip()
    if casefold:
        s = s.casefold()
    
    if yo2e:
        s = s.replace('—ë', '–µ').replace('–Å', '–µ')

    s = " ".join(s.split())
    return s

print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t")) # "–ø—Ä–∏–≤–µ—Ç, –º–∏—Ä"
print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞")) # "–µ–∂–∏–∫, –µ–ª–∫–∞"
print(normalize("Hello\r\nWorld")) # "hello world"
print(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ")) # "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 1](../../images/lab03/img01.png)
—Ñ—É–Ω–∫—Ü–∏—è tokenize
```python
def tokenize(text: str) -> list[str]: # 'hello, world'
    result = []
    word = ''
    for i, w in enumerate(text):
        if w.isalnum() or w == '_':
            word += w
        elif w == '-' and word and i + 1 < len(text) and text[i+1].isalnum():
            word += w
        else:
            if word:
                result.append(word)
                word = ''
    if word:
        result.append(word)
    return result

print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
print(tokenize("hello,world!!!"))
print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))
print(tokenize("2025 –≥–æ–¥"))
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))
```

![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 2](../../images/lab03/img02.png)
—Ñ—É–Ω–∫—Ü–∏—è top_n
```python
def count_freq(tokens: list[str]) -> dict[str, int]:
    result = {}
    for word in tokens:
        result[word] = result.get(word, 0) + 1
    return result

print(count_freq(["a","b","a","c","b","a"]))
print(count_freq(["bb", "aa", "bb", "aa", "cc"]))
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 3](../../images/lab03/img03.png)

—Ñ—É–Ω–∫—Ü–∏—è count_freq
```python

```
### –ó–∞–¥–∞–Ω–∏–µ –í
