## –ó–∞–¥–∞–Ω–∏–µ –ê
#### —Ñ—É–Ω–∫—Ü–∏—è normalize
```python
def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    s = text.strip()
    if casefold:
        s = s.casefold()
    
    if yo2e:
        s = s.replace('—ë', '–µ').replace('–Å', '–µ')

    s = " ".join(s.split())
    return s

print(normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t")) # "–ø—Ä–∏–≤–µ—Ç, –º–∏—Ä"
print(normalize("—ë–∂–∏–∫, –Å–ª–∫–∞")) # "–µ–∂–∏–∫, –µ–ª–∫–∞"
print(normalize("Hello\r\nWorld")) # "hello world"
print(normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ")) # "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 1](../../images/lab03/img01.png)

#### —Ñ—É–Ω–∫—Ü–∏—è tokenize
```python
def tokenize(text: str) -> list[str]:
    result = []
    word = ''
    for i, w in enumerate(text):
        if w.isalnum() or w == '_':
            word += w
        elif w == '-' and word and i + 1 < len(text) and text[i+1].isalnum():
            word += w
        else:
            if word:
                result.append(word)
                word = ''
    if word:
        result.append(word)
    return result

print(tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"))
print(tokenize("hello,world!!!"))
print(tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"))
print(tokenize("2025 –≥–æ–¥"))
print(tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ"))
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 2](../../images/lab03/img02.png)
<<<<<<< HEAD
—Ñ—É–Ω–∫—Ü–∏—è count_freq
=======

#### —Ñ—É–Ω–∫—Ü–∏—è count_freq
>>>>>>> c7122c7 (n)
```python
def count_freq(tokens: list[str]) -> dict[str, int]:
    result = {}
    for i in tokens:
        result[i] = result.get(i, 0) + 1
    return result

print(count_freq(["a","b","a","c","b","a"]))
print(count_freq(["bb", "aa", "bb", "aa", "cc"]))
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 3](../../images/lab03/img03.png)

#### —Ñ—É–Ω–∫—Ü–∏—è top_n
```python
def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    items = list(freq.items())
<<<<<<< HEAD

    # c–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø—É–∑—ã—Ä—å–∫–æ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã,
    # –∞ –ø—Ä–∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ ‚Äî –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É —Å–ª–æ–≤–∞
    for i in range(len(items)):
        for j in range(len(items) - i - 1):
            word1, count1 = items[j]
            word2, count2 = items[j + 1]

            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ
            if count1 < count2:
                items[j], items[j + 1] = items[j + 1], items[j]
            # –ï—Å–ª–∏ —á–∞—Å—Ç–æ—Ç—ã —Ä–∞–≤–Ω—ã ‚Äî —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
            elif count1 == count2 and word1 > word2:
                items[j], items[j + 1] = items[j + 1], items[j]

=======
    items.sort(key=lambda x: (-x[1], x[0]))  # —Å–Ω–∞—á–∞–ª–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–µ, –ø–æ—Ç–æ–º –ø–æ —Å–ª–æ–≤—É
>>>>>>> 03b9ebc (new)
    return items[:n]

print(top_n({"a":3,"b":2,"c":1}, n=2)) # [('a', 3), ('b', 2)]
print(top_n({"aa":2,"bb":2,"cc":1}, n=2)) # [('aa', 2), ('bb', 2)]
print(top_n({"x":5,"y":5,"a":5}, n=3))
```

![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 4](../../images/lab03/img04.png)


## –ó–∞–¥–∞–Ω–∏–µ –í
–≤–≤–æ–¥–∏–º –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª —Å—Ç—Ä–æ—á–∫—É –∏–∑ —Å–ª–æ–≤ –∏ –Ω–∞–∂–∏–º–∞–µ–º control + D
```python
import sys, os # –ù–ê–ñ–ê–¢–¨ control + D 
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))
from src.lib.text import normalize, tokenize, count_freq, top_n

def main():
    # —á–∏—Ç–∞–µ–º –≤–µ—Å—å –≤–≤–æ–¥ (–¥–æ EOF)
    text = sys.stdin.read()

    # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    norm_text = normalize(text)

    # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = tokenize(norm_text)

    # —á–∞—Å—Ç–æ—Ç—ã
    freq = count_freq(tokens)

    # —Ç–æ–ø 5
    top = top_n(freq, n=5)

    print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {len(tokens)}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(freq)}")
    print("–¢–æ–ø-5:")
    for word, count in top:
        print(f"{word}:{count}")

if __name__ == "__main__":
    main()
```
![–ö–∞—Ä—Ç–∏–Ω–∫–∞ 5](../../images/lab03/img05.png)
